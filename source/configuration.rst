====================
 設定を変更してみる
====================

``jubaclassifier`` を起動する際に、 ``-f`` オプションで設定ファイルを渡したのを覚えているでしょうか。
起動時の設定には、どのような学習を行うか、どのようなパラメータ設定で行うかなどの重要なチューニングポイントが隠れています。
次に、この設定を修正することで機械学習のチューニングを行います。


設定を見てみる
==============

まずは設定ファイルの全体像をみてみましょう。

::

   $ cat /opt/jubatus/share/jubatus/example/config/classifier/pa1.json
   {
     "converter" : {
       "string_filter_types" : {},
       "string_filter_rules" : [],
       "num_filter_types" : {},
       "num_filter_rules" : [],
       "string_types" : {},
       "string_rules" : [
         { "key" : "*", "type" : "str", "sample_weight" : "bin", "global_weight" : "bin" }
       ],
       "num_types" : {},
       "num_rules" : [
         { "key" : "*", "type" : "num" }
       ]
     },
     "parameter" : {
       "regularization_weight" : 1.0
     },
     "method" : "PA1"
   }

長くてよくわかりませんね。
Jubatusの全てのサーバーの設定ファイルは、単一のJSONフォーマットです。
Jubatusの機械学習器としての振る舞いは、この中で設定されています。

.. csv-table::
   :header: "フィールド名", "説明"

   converter, 特徴変換の方法を指定します。
   parameter, 学習アルゴリズムに渡すパラメータを指定します。
   method, 利用する学習アルゴリズム名を指定します。

それぞれのパラメータを変更しながら、その役割を順番に説明していきます。
フォーマットの関係で順序が前後しますが、下から順番に解説します。

また、各設定項目の詳細は、 `ドキュメント <http://jubat.us/ja/api_classifier.html>`_ を参照してください。


学習アルゴリズムを変えてみる
============================

教師付きデータがやってきた時に、機械学習の内部状態をどのように変更するのかを決めるのが学習アルゴリズムです。
Jubatusでは、データが1つやってくるたびに内部状態を変える、 *オンライン学習* という方式のアルゴリズムをサポートしています。

設定ファイルの中では、 ``method`` の項目が学習アルゴリズムを指定しています。
まずはサンプルの設定を適当なディレクトリにコピーしてきましょう。
ここから、この設定ファイルをいじっていきます。
最初の設定では "PA1" という手法をとっていましたが、これを "AROW" に変えてみましょう。

::

   $ cp /opt/jubatus/share/jubatus/example/config/classifier/pa1.json ./my_conf.json
   $ vi my_conf.json
   $ jubaclassifier -f my_conf.json

同様に ``gender.py`` を実行してみましょう。
実行結果の幾つかが変わりました。

"PA1" というのは Passive Aggressive という手法で、2003年に初めて提案されました。
また、 "AROW" は Adaptive Regularization of Weight Vectors という手法で、2009年に提案されました。
機械学習のアルゴリズム自体は年々進歩しています。
両者は問題設定こそ同じですが、学習効率の向上やノイズデータへの耐性向上を果たしています。
もちろん両者のアルゴリズムの複雑性は違い、概ね新しいアルゴリズムのほうが複雑担っています。

その他にも NHERD や CW などの手法も利用できますが、詳細はドキュメントに譲ります。
他の手法も試して、精度にどのような影響があるのか確認するのも良いでしょう。
なお、利用するアルゴリズムは実行途中で変えることができません。


パラメータを変えてみる
======================

一般に、学習アルゴリズムは内部に大量の（数万、数十万という様なオーダー）パラメータを持っており、教師データにもとづいてこれを調整します。
しかし、多くのアルゴリズムはこれとは別に、学習前にパラメータを渡します。
この学習前に与えるパラメータは、どのくらい積極的に学習するかなど、学習自体の制御に使われます。
教師データを使って調整されないため、自動調整されるパラメータとは区別されて *ハイパーパラメータ* と呼ばれることもあります。

ハイパーパラメータの役割はいくつかありますが、現在分類器にあるハイパーパラメータは大雑把に言うとデータに対する感度の意味合いを持っています。
感度が高いと、学習は早く進む代わりにノイズに弱くなります。
一方、感度が低いと、学習が遅くなる代わりにノイズに負けなくなります。
このトレードオフの調整は難しく、実験的に良い塩梅のパラーメータを探ることがよく行なわれます。

では、ハイパーパラメータを上下させてみましょう
設定ファイルの ``regularization_weight`` を変えて実行してみましょう。

::

   ...
   "parameter" : {
     "regularization_weight" : 10.0
   },
   ...

この状態で実行すると、結果が変わります。
だいたい、10倍ごとぐらいの粗い粒度で試すことが多いです。

適切なハイパーパラメータはデータの種類やデータの量によっても変わります。
また、ハイパーパラメータも自動で調整する様な手法もあるのですが、Jubatusにはまだ実装されていません。


特徴抽出の設定を変えてみる
==========================

残りの設定、すなわち ``converter`` の部分は特徴抽出の設定です。
この部分が、設定の中で一番大きいです。
特徴抽出は解析がうまくいくかどうかを左右する非常に重要なポイントなので詳しく説明します。

一般的に、機械学習の技術は入力データとしてテキストや画像といった生の情報を扱いません。
普通は数値情報に落ちた、ベクトルの形式のデータを扱うことがほとんどです。
では、どうやって文や画像、行動履歴などのデータを扱うのでしょう？
この間に入るのが特徴抽出（あるいは特徴変換）といわれる処理です。
入力データと解析対象に応じて、入力データの生の文や画像はベクトル形式に変換されます。
機械学習技術の多くがベクトルデータを入力として仮定しているため、一度ベクトルデータに変換してしまえば元々の入力が文であっても画像であっても同じように処理ができます。

普通の機械学習ライブラリではこの特徴抽出の仕組を備えていません。
そのため、ユーザーは特徴抽出処理を自分で書かなければなりませんでした。
Jubatusではこの特徴抽出処理の仕組みも備えているため、ユーザーは生のデータを直接Jubatusに入力しても機械学習を利用できるのです。


デフォルトの設定の解説
----------------------

デフォルトの設定を見ながら、どのような処理がなされるか解説します。
処理の流れの概要が頭のなかに入っていたほうが、残りの理解が進むでしょう。

まず、Jubatusへの入力データについてです。
Jubatusには様々な種類の非構造データを入れられることを目指しており、現状では3種類のデータを扱えます。
1つは文書などを始めとする文字列です。
2つ目はセンサーのデータなどの数値情報です。
3つ目は画像や音声などのバイナリデータです。
各データに必要な特徴抽出処理は異なるため、全く別の系統で処理が行われます。

以下では文字列と数値情報のみを扱います。
例として、次のような情報がやってくると仮定します。

::

   {
     "hair": "short",
     "top": "T shirt",
     "bottom": "jeans",
     "height": 1.70
   }

さて、機械学習でデータを扱うには、裏ではベクトル形式に変換しなければなりません。
普通ベクトルといえば、 (1.5, 2.3, 4.2) の様に数値の列で表されますが、ここではベクトルの次元とその値のペアの集合であらわします。
内部的にはもっとたくさんの次元があって、明記されない次元は 0 であるとして処理します。
上の文字列情報を含んだデータを単純にベクトル化してみます。

::

   {
     "hair=short": 1.0,
     "top=T shirt": 1.0,
     "bottom=jeans": 1.0,
     "height": 1.70
   }

文字列に対する処理と、数値に対する処理が異なることに気づくかとおもいます。
順に説明します。

文字列に対する処理は、統計学でいうところの質的変数をダミー変数に変える処理を行なっているということです。
この変換規則を記述しているのが、Jubatusの特徴変換の設定中の ``string_rules`` になります。
デフォルトの設定ではどうなっているでしょうか。

::

   ...
       "string_rules" : [
         { "key" : "*", "type" : "str", "sample_weight" : "bin", "global_weight" : "bin" }
       ],
   ...

この設定がいわんとしていることは、以下の4つです。

1. key: "*" は全てのキーの情報に対して処理するという意味です。
2. type: "str" は文字列情報をそのまま1つの次元とするという意味です。
3. sample_weight: "bin" は重みを1.0にするという意味です。
4. global_weight: "bin" は重みを1.0にするという意味です。

keyの値でマッチした入力データに対して、typeで指定した特徴抽出を行うということです。
残りの2つは重み付けの方法です。
sample_weightはデータ中の出現回数情報をどう使うかで、"bin" なら出現すれば 1 しなければ 0 とします。
global_weightはデータ中の出現回数以外の情報での重み付けで、"bin"なら常に1です。
実際のベクトルの値はsample_weightとglobal_weightの積で求めます（結果的に1.0になります）。

数値情報である身長はどのように処理されているでしょう。
こちらの変換規則は ``num_rules`` に記述されています。

::

  ...
    "num_rules" : [
      { "key" : "*", "type" : "num" }
    ]
  ...

これも先と同様です。

1. key: "*" は全てのキーの情報に対して処理するという意味です。
2. type: "num" は数値情報をそのままの値として利用するという意味です。

typeの中に重み付けの方法も含まれるため、こちらの設定はシンプルです。
上記の設定ですと、与えられた1.70という数値がそのままベクトル情報になります。


以上の特徴抽出ルールを工夫すると、元データの異なる側面を捉えられるようになります。
次は特徴の取り方の工夫をします。


特徴の取り方を工夫する
----------------------

特徴抽出を工夫する例として、より多くの情報が入っている場合の例を示します。
住所の情報は非常に細かくなっています。

::

   {
     "名前": "山田 太郎",
     "住所": "東京都 文京区 本郷"
   }

このままですと、住所の情報は粒度が細かすぎます。
例えば「"東京"に住んでいる人は若者が多い」のような、もう少し粒度の粗い情報で学習したくなります。
つまり、以下の様な形に変換したくなります。

::

  {
    "名前=山田 太郎": 1.0,
    "住所=東京都": 1.0,
    "住所=文京区": 1.0
    "住所=本郷": 1.0
  }

そこで、 ``情報`` の情報をスペース区切りにしてみましょう。
先ほど解説した通り、どの特徴抽出処理をするか指定するのが、 ``string_rules`` でした。
``string_rules`` に、スペース区切りで特徴抽出する ``space`` の規則を追加します。

::

   ...
       "string_rules" : [
         { "key" : "name", "type" : "str",
           "sample_weight" : "bin", "global_weight" : "bin" },
         { "key" : "address", "type" : "space",
           "sample_weight" : "bin", "global_weight" : "bin" }
       ],
   ...

抽出規則を ``str`` から ``space`` に変更したところに注目してください。
さて実行してみましょう。
もともとの変換ですと、「"東京都 文京区 本郷"の人は若者が多い」という様な、粒度の細かい学習しか出来ませんでした。
この変更の効果は、「"東京都"の人は若者が多い」という、もう少し大雑把な粒度の情報も学習できるようになります。

特徴抽出の基本的な考え方は、細かい粒度で特徴を取るか、粗い粒度で特徴を取るのかの調整です。
細かく取るほど、細かい違いを学習できる可能性が高まりますが、大きな傾向は捉えられなくなり、結果的に必要な教師データの数が増えます。
逆に、粗い粒度の情報だけ使うと全体の傾向がすぐに学習される変わりに、細かい違いに鈍感になります。
どちらが良いかはアプリケーションやデータによって異なるため一概には言えませんが、自然言語の場合は概ね単語くらいの単位が経験的にはよく機能しています。
実際には、単語の共起など更に複雑な特徴を利用する場合もありますが、今回は割愛します。


プラグインを利用する
--------------------

今回は、スペース区切りで特徴を使う場合だけ試しました。
複雑な設定もたくさんありますが、もう1つだけ紹介して残りはドキュメントに譲ります。

自由文でデータが与えられると、スペースで区切っても適切な特徴は取れません。
そこで利用するのが自然言語処理技術です。
特にここで利用するのが、形態素解析と呼ばれる技術で、大雑把には文を単語の列に分解する技術です。
JubatusではオープンソースのMeCabという形態素解析器を使って入力文を単語に分割します。
ちなみにMeCabの中でも機械学習技術は応用されています。

設定の書き方は少し複雑です。
MeCabを利用するときはプラグインとして利用する必要があります。
プラグインのロードは、 ``string_types`` の項でプラグインの読み込み方法を記述して、読み込んだプラグインの適用ルールを ``string_rules`` で記述する寸法です。
以下に例を書きます。

::

   ...
   "string_types": {
   "mecab": {
       "method": "dynamic",
       "path": "libmecab_splitter.so",
       "function": "create",
     }
   },
   "string_rules" : [
     { "key" : "*", "type" : "mecab", "sample_weight" : "bin", "global_weight" : "bin" }
   ],
   ...


プラグインは自作することもできるので、興味のある方は自作にもチャレンジしてください。


数値データや他の設定の仕方
--------------------------

数値データの扱いも、先程までの設定の ``string`` を ``num`` に変えるとほぼ同じように設定出来ます。
詳細はドキュメントに譲りますが、こちらも色々な設定で精度が変わってきます。

他に、今回は解説しませんが特徴抽出の前に要らない情報を除去するなどのフィルター処理などを行うこともできます。
例えばHTMLタグを除去したり、定型フォーマットの箇所を削除するなどの用途に使えます。
