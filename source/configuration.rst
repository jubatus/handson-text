====================
 設定を変更してみる
====================

``jubaclassifier`` を起動する際に、 ``-f`` オプションで設定ファイルを渡したのを覚えているでしょうか。
起動時の設定には、どのような学習を行うか、どのようなパラメータ設定で行うかなどの重要なチューニングポイントが隠れています。
次に、この設定を修正することで機械学習のチューニングを行います。

まずは設定ファイルの全体像をみてみましょう。

::

   TODO ディレクトリの場所確認
   $ cat /usr/local/share/jubatus/example/config/classifier/pa1.json
   {
     "converter" : {
       "string_filter_types" : {},
       "string_filter_rules" : [],
       "num_filter_types" : {},
       "num_filter_rules" : [],
       "string_types" : {},
       "string_rules" : [
         { "key" : "*", "type" : "str", "sample_weight" : "bin", "global_weight" : "bin" }
       ],
       "num_types" : {},
       "num_rules" : [
         { "key" : "*", "type" : "num" }
       ]
     },
     "parameter" : {
       "regularization_weight" : 1.0
     },
     "method" : "PA1"
   }

長くてよくわかりませんね。
Jubatusの全てのサーバーの設定ファイルは、単一のJSONフォーマットです。
Jubatusの機械学習器としての振る舞いは、この中で設定されています。

.. csv-table::
   :header: "フィールド名", "説明"

   method, 利用する学習アルゴリズム名を指定します。
   parameter, 学習アルゴリズムに渡すパラメータを指定します。
   converter, 特徴変換の方法を指定します。

それぞれのパラメータを変更しながら、その役割を順番に説明していきます。

また、各設定項目の詳細は、 `ドキュメント <http://jubat.us/ja/api_classifier.html>`_ を参照してください。


学習アルゴリズムを変えてみる
============================

教師付きデータがやってきた時に、機械学習の内部状態をどのように変更するのかを決めるのが学習アルゴリズムです。
Jubatusでは、データが1つやってくるたびに内部状態を変える、 *オンライン学習* という方式のアルゴリズムをサポートしています。

設定ファイルの中では、 ``method`` の項目が学習アルゴリズムを指定しています。
最初の設定では "PA1" という手法をとっていましたが、これを "AROW" に変えてみましょう。

::

   $ cp /usr/local/share/jubatus/example/config/classifier/pa.json ./my_conf.json
   $ vi my_conf.json
   $ jubaclassifier -f my_conf.json

実行結果の幾つかが変わりました。

"PA1" というのは Passive Aggressive という手法で、2003年に初めて提案されました。
また、 "AROW" は Adaptive Regularization Of Weight vector という手法で、2009年に提案されました。
このアルゴリズムの詳細は理解しなくても使えます。
おまけで少しだけ紹介すると、 PA-1 は以下の更新式を使って内部パラメータを更新します。

一方の AROW では以下の更新式を使います。


その他にも NHERD や CW などの手法も利用できますが、詳細はドキュメントに譲ります。
他の手法も試して、精度にどのような影響があるのか確認するのも良いでしょう。
なお、利用するアルゴリズムは実行途中で変えることができません。


パラメータを変えてみる
======================

一般に、学習アルゴリズムは内部に大量の（数万、数十万という様なオーダー）パラメータを持っており、教師データにもとづいてこれを調整します。
しかし、多くのアルゴリズムはこれとは別に、学習前にパラメータを渡します。
この学習前に与えるパラメータは、どのくらい積極的に学習するかなどの制御に使われます。
教師データを使って調整されないため、自動調整されるパラメータとは区別されて *ハイパーパラメータ* と呼ばれることもあります。

ハイパーパラメータの役割はいくつかありますが、現在分類器にあるハイパーパラメータはデータに対する感度の制御に使われています。
感度が高いと、学習は早く進む代わりにノイズに弱くなります。
一方、感度が低いと、学習が遅くなる代わりにノイズに負けなくなります。
このトレードオフの調整は難しく、実験的に良い塩梅のパラーメータを探ることがよく行なわれます。

では、ハイパーパラメータを上下させてみましょう
設定ファイルの ``regularization_weight`` を変えてみましょう。

TODO

この状態で実行すると、結果が変わります。

適切なハイパーパラメータはデータの種類やデータの量によっても変わります。
また、ハイパーパラメータも自動で調整する様な手法もあるのですが、Jubatusにはまだ実装されていません。


特徴抽出の設定を変えてみる
==========================

残りの設定、すなわち ``converter`` の部分は特徴抽出の設定です。
この部分が、設定中で一番大きいです。
特徴抽出は解析がうまくいくかどうかを左右する非常に重要なポイントなので詳しく説明します。

一般的に、機械学習の技術は入力データとしてテキストや画像といった生の情報を扱いません。
普通は数値情報に落ちた、ベクトルの形式のデータを扱うことがほとんどです。
では、どうやって文や画像、行動履歴などのデータを扱うのでしょう？
この間に入るのが特徴抽出（あるいは特徴変換）といわれる処理です。
入力データと解析対象に応じて、入力データの生の文や画像はベクトル形式に変換されます。
機械学習技術の多くがベクトルデータを入力として仮定しているため、一度ベクトルデータに変換してしまえば元々の入力が文であっても画像であっても同じように処理ができます。

TODO

普通の機械学習ライブラリではこの特徴抽出の仕組を備えていません。
そんため、ユーザーは特徴抽出処理を自分で書かなければなりませんでした。
Jubatusではこの特徴抽出処理の仕組みも備えているため、ユーザーは生のデータを直接Jubatusに入力しても機械学習を利用できるのです。


デフォルトの設定の解説
----------------------

デフォルトの設定を見ながら、どのような処理がなされるか解説します。
処理の流れの概要が頭のなかに入っていたほうが、残りの理解が進むでしょう。

まず、Jubatusへの入力データについてです。
Jubatusには様々な種類の非構造データを入れられることを目指してはいますが、現状では2種類のデータしか扱えません。
1つは文書などを始めとする文字列です。
もう1つはセンサーのデータなどの数値情報です。
両者に必要な特徴抽出処理は異なるため、全く別の系統で処理が行われます。
前者の文字列情報がやってくることを仮定して説明していきます。

例として、以下の様な情報がやってくると仮定します。

::

   {
     "name": "山田 太郎",
     "address": "東京都 文京区"
   }

上記のデータは説明のための形式なので、JSONをそのままJubatusが処理できるわけではないことには気をつけてください。
さて、機械学習でデータを扱うには、裏ではベクトル形式に変換しなければなりません。


::

  {
    "name=山田 太郎": 1.0,
    "address=東京都 文京区": 1.0
  }
  

これは統計学でいうところの、質的変数をダミー変数に変える処理を行なっているということです。



特徴の取り方を工夫する
----------------------

先のデータを良く見てみましょう。
住所の情報は非常に細かくなっています。

::

   {
     "name": "山田 太郎",
     "address": "東京都 文京区"
   }

このままですと、各質的変数の粒度が細かすぎます。
例えば「"東京"に住んでいる人は病気になりやすい」のような、もう少し粒度の粗い情報で学習したくなります。

::

  {
    "name=山田 太郎": 1.0,
    "address=東京都": 1.0,
    "address=文京区": 1.0
  }

そこで、 ``address`` 情報をスペース区切りにしてみましょう。
先ほど解説した通り、どの特徴抽出処理をするか指定するのが、 ``string_rules`` でした。
``string_rules`` に、スペース区切りで特徴抽出する ``space`` の規則を追加します。


::

   ...
       "string_rules" : [
         { "key" : "name", "type" : "str", "sample_weight" : "bin", "global_weight" : "bin" },
         { "key" : "address", "type" : "space", "sample_weight" : "bin", "global_weight" : "bin" }
       ],
   ...

抽出規則を ``str`` から ``space`` に変更したところに注目してください。
さて実行してみましょう。
この変更の効果は、「東京都であればTODO」という傾向を学習することです。
したがって、例えばTODOは "東京都 杉並区" ですが、TODOに分類されました。

特徴抽出の基本的な考え方は、細かい粒度で特徴を取るか、粗い粒度で特徴を取るのかの調整です。
細かく取るほど、細かい違いを学習できる可能性が高まりますが、大きな傾向は捉えられなくなり、結果的に必要な教師データの数が増えます。
逆に、粗い粒度の情報だけ使うと全体の傾向がすぐに学習される変わりに、細かい違いに鈍感になります。
どちらが良いかはアプリケーションやデータによって異なるため一概には言えませんが、概ね単語くらいの単位が経験的にはよく機能しています。
実際には、単語の共起など更に複雑な特徴を利用する場合もありますが、今回は割愛します。


さらに複雑な設定
----------------

今回は、スペース区切りで特徴を使う場合だけ試しました。
TODO
